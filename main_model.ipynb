{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:19:54.428273Z",
     "iopub.status.busy": "2022-12-13T18:19:54.427895Z",
     "iopub.status.idle": "2022-12-13T18:19:54.435266Z",
     "shell.execute_reply": "2022-12-13T18:19:54.434093Z",
     "shell.execute_reply.started": "2022-12-13T18:19:54.428242Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= pd.read_excel(\"./model_data1.xlsx\")\n",
    "df1 = df1.fillna(df1.mean())\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_excel(\"./model_data2.xlsx\")\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= pd.concat([df1[df1.columns[:-1]],df2],axis=1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化变量相关性\n",
    "\n",
    "col = [\"front_PCA1\", \"Roof subsidence\", \"Surface elevation\" , \"Burial depth\", \"side_tSNE1\", \"side_PCA5\", \"front_PCA5\",\"side_PCA2\",\"front_tSNE3\",\"Maximum velocity of shock wave\",\"Roof tensile stress\"]\n",
    "# 计算相关性矩阵\n",
    "df4  = df3[col]\n",
    "correlation_matrix = df4.corr()\n",
    "print(correlation_matrix )\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# 绘制热力图\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(correlation_matrix, \n",
    "                 annot=False, fmt=\".2f\", cmap='RdBu_r',  # 设置annot=False, 不自动注释\n",
    "                 linewidths=0.5, linecolor='white', \n",
    "                 cbar_kws={'shrink': 0.8})  # 缩小颜色条\n",
    "\n",
    "# 手动添加相关系数数值\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(correlation_matrix.shape[1]):\n",
    "        ax.text(j + 0.5, i + 0.5, f\"{correlation_matrix.iloc[i, j]:.2f}\",\n",
    "                ha='center', va='center', \n",
    "                fontsize=12, fontweight='bold', color='black')\n",
    "\n",
    "# 增大X、Y轴标签的字体大小并加粗\n",
    "plt.xticks(fontsize=18, fontweight='bold')\n",
    "plt.yticks(fontsize=18, fontweight='bold')\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig('./correlation_matrix.png', dpi=800)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "describe_df = df3.describe().T\n",
    "describe_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_df.to_csv('describe_df.csv')\n",
    "print(\"describe_df.csv is saved in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Define Model Evaluation Metric Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from ngboost import NGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "def evaluate_regression_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Function to evaluate the performance of a regression model.\n",
    "\n",
    "    Parameters:\n",
    "        model: The trained model to evaluate.\n",
    "        X_test (np.ndarray): Features of the test dataset.\n",
    "        y_test (np.ndarray): True labels of the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'R^2': r2_score(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'MBE': np.mean(y_test - y_pred),\n",
    "        'AIC': len(y_test) * np.log(mean_squared_error(y_test, y_pred)) + 2 * X_test.shape[1],\n",
    "        'BIC': len(y_test) * np.log(mean_squared_error(y_test, y_pred)) + np.log(len(y_test)) * X_test.shape[1],\n",
    "        'SI': np.sqrt(mean_squared_error(y_test, y_pred)) / np.mean(y_test),\n",
    "        'KGE': 1 - np.sqrt((r2_score(y_test, y_pred) - 1) ** 2 + (np.std(y_pred) / np.std(y_test) - 1) ** 2 + (np.mean(y_pred) / np.mean(y_test) - 1) ** 2)\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Split Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def data_loader(df):\n",
    "    X = df[df.columns[:-1]]\n",
    "    y = df[df.columns[-1]]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=12)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Step 2.3: Perform Hyperparameter Optimization (Train the Model and Save the Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "param_grid_knn = {'n_neighbors': [3, 5, 7, 9, 10, 15, 20]}\n",
    "param_grid_xgb = {'max_depth': [2, 3, 5, 10, 15, 20, 50], 'learning_rate': [0.001, 0.01, 0.1,1], 'n_estimators': [10, 20, 30, 50, 100, 200, 300], 'subsample': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]}\n",
    "param_grid_rf = {'n_estimators': [10, 20, 30, 50, 100, 150, 200, 300, 400], 'max_depth': [3, 5, 7, 10, 15, 20, 50, 100], 'min_samples_split': [2, 4, 6, 8]}\n",
    "param_grid_ann = {'hidden_layer_sizes': [(3,), (5,), (3, 3), (5, 5), (10,), (10, 10), (20,), (20, 20), (50,), (100,), (50, 50), (100, 100)], 'activation': ['tanh', 'relu'], 'alpha': [0.0001, 0.001, 0.01, 0.1, 10], 'max_iter': [200, 500, 1000]}\n",
    "param_grid_lasso = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
    "param_grid_elasticnet = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "param_grid_adaboost = {'n_estimators': [50, 100, 200, 300], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "\n",
    "\n",
    "svr_model = SVR()\n",
    "knn_model = KNeighborsRegressor()\n",
    "xgb_model = xgb.XGBRegressor(use_label_encoder=False, eval_metric='rmse')\n",
    "rf_model = RandomForestRegressor()\n",
    "ann_model = MLPRegressor()\n",
    "lasso_model = Lasso()\n",
    "elasticnet_model = ElasticNet()\n",
    "adaboost_model = AdaBoostRegressor()\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Lasso': (lasso_model, param_grid_lasso),\n",
    "    'KNN': (knn_model, param_grid_knn),\n",
    "    'XGBoost': (xgb_model, param_grid_xgb),\n",
    "    'RandomForest': (rf_model, param_grid_rf),\n",
    "    'AdaBoost': (adaboost_model, param_grid_adaboost) ,\n",
    "    'ANN': (ann_model, param_grid_ann),\n",
    "    'ElasticNet': (elasticnet_model, param_grid_elasticnet)\n",
    "    \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datalist = [df1,df2,df3]\n",
    "for i in range(len(datalist)):\n",
    "    X_train, X_test, y_train, y_test = data_loader(datalist[i])\n",
    "    data_name = \"df\"+str(i+1)\n",
    "    eval_results = []\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"\\n训练{data_name} - {model_name}...\")\n",
    "        if param_grid:\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            joblib.dump(best_model, f'./{data_name}_{model_name}.pkl')\n",
    "            print(f\"{model_name} best_params ：{grid_search.best_params_}\")\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            best_model = model\n",
    "            joblib.dump(best_model, f'./{model_name}.pkl')\n",
    "        \n",
    "\n",
    "        metrics = evaluate_regression_model(best_model, X_test, y_test)\n",
    "        metrics['Model'] = model_name\n",
    "        eval_results.append(metrics)\n",
    "        print(f\"{model_name} evaluation result：\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "\n",
    "    eval_result_df = pd.DataFrame(eval_results)\n",
    "    eval_result_df.to_excel(f'./{data_name}_model_evaluation_results.xlsx', index=False)\n",
    "\n",
    "    print(f\"{data_name}all model evaluation results saved in {data_name}_model_evaluation_results.xlsx。\")\n",
    "\n",
    "print(\"train is ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df1 = pd.read_excel(\"./df1_model_evaluation_results.xlsx\")\n",
    "eval_result_df1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df2 = pd.read_excel(\"./df2_model_evaluation_results.xlsx\")\n",
    "eval_result_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df3 = pd.read_excel(\"./df3_model_evaluation_results.xlsx\")\n",
    "eval_result_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Validate Data Input Effectiveness (Using Non-Parametric Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **T-Test**\n",
    "The T-test is used to determine whether there is a significant difference between the means of two groups. It is particularly suitable when:\n",
    "- The data follows a **normal distribution**.\n",
    "- The variances between the two groups are **homogeneous** (similar).\n",
    "\n",
    "### Key Applications:\n",
    "- Comparing model performance on different datasets.\n",
    "- Assessing whether feature engineering steps improve prediction accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **Wilcoxon Test**\n",
    "The Wilcoxon test is a **non-parametric test** that is ideal for scenarios where:\n",
    "- The data **does not follow a normal distribution**.\n",
    "- The sample size is **small**.\n",
    "\n",
    "### Key Characteristics:\n",
    "- It compares the **medians** of two groups to assess significant differences.\n",
    "- Useful for evaluating metrics where the distribution is unknown or deviates from normality.\n",
    "\n",
    "### Common Use Cases:\n",
    "- Testing input feature effectiveness when data distributions are skewed.\n",
    "- Assessing robustness of model predictions under varying input conditions. \n",
    "\n",
    "By applying these tests appropriately, we can ensure rigorous validation of model performance and input feature reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3.1: Perform Validation on Basic Features and Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, wilcoxon, shapiro\n",
    "\n",
    "metrics = [\"R^2\", \"RMSE\", \"MAE\", \"MBE\", \"AIC\", \"BIC\", \"SI\",\"KGE\"]\n",
    "results = pd.DataFrame(index=metrics, columns=[\"T-Test p-value\", \"Wilcoxon p-value\", \"Normality p-value (df1)\", \"Normality p-value (df2)\"])\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    norm_p_val_df1 = shapiro(eval_result_df1[metric])[1]\n",
    "    norm_p_val_df2 = shapiro(eval_result_df2[metric])[1]\n",
    "\n",
    "\n",
    "    if norm_p_val_df1 > 0.05 and norm_p_val_df2 > 0.05: \n",
    "\n",
    "        t_stat, p_val_t = ttest_ind(eval_result_df1[metric], eval_result_df2[metric])\n",
    "        results.loc[metric, \"T-Test p-value\"] = p_val_t\n",
    "    else:\n",
    "\n",
    "        w_stat, p_val_w = wilcoxon(eval_result_df1[metric], eval_result_df2[metric])\n",
    "        results.loc[metric, \"Wilcoxon p-value\"] = p_val_w\n",
    "\n",
    "    results.loc[metric, \"Normality p-value (df1)\"] = norm_p_val_df1\n",
    "    results.loc[metric, \"Normality p-value (df2)\"] = norm_p_val_df2\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Perform Validation on Basic Features and Merged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "\n",
    "    norm_p_val_df1 = shapiro(eval_result_df1[metric])[1]\n",
    "    norm_p_val_df2 = shapiro(eval_result_df3[metric])[1]\n",
    "\n",
    "\n",
    "    if norm_p_val_df1 > 0.05 and norm_p_val_df2 > 0.05:\n",
    "\n",
    "        t_stat, p_val_t = ttest_ind(eval_result_df1[metric], eval_result_df3[metric])\n",
    "        results.loc[metric, \"T-Test p-value\"] = p_val_t\n",
    "    else:\n",
    "\n",
    "        w_stat, p_val_w = wilcoxon(eval_result_df1[metric], eval_result_df2[metric])\n",
    "        results.loc[metric, \"Wilcoxon p-value\"] = p_val_w\n",
    "\n",
    "    results.loc[metric, \"Normality p-value (df1)\"] = norm_p_val_df1\n",
    "    results.loc[metric, \"Normality p-value (df2)\"] = norm_p_val_df2\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Perform Validation on Image Features and Merged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "\n",
    "    norm_p_val_df1 = shapiro(eval_result_df2[metric])[1]\n",
    "    norm_p_val_df2 = shapiro(eval_result_df3[metric])[1]\n",
    "\n",
    "\n",
    "    if norm_p_val_df1 > 0.05 and norm_p_val_df2 > 0.05:  \n",
    "\n",
    "        t_stat, p_val_t = ttest_ind(eval_result_df2[metric], eval_result_df3[metric])\n",
    "        results.loc[metric, \"T-Test p-value\"] = p_val_t\n",
    "    else:\n",
    "\n",
    "        w_stat, p_val_w = wilcoxon(eval_result_df1[metric], eval_result_df2[metric])\n",
    "        results.loc[metric, \"Wilcoxon p-value\"] = p_val_w\n",
    "\n",
    "    results.loc[metric, \"Normality p-value (df1)\"] = norm_p_val_df1\n",
    "    results.loc[metric, \"Normality p-value (df2)\"] = norm_p_val_df2\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Visualization of Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.hist(df3[df3.columns[-1]], bins=20, color='blue', edgecolor='black')  \n",
    "plt.title('Histogram')  \n",
    "plt.xlabel('Values')  \n",
    "plt.ylabel('Frequency') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "colors = ['black', 'red', 'yellow', 'green', 'cyan', 'blue']\n",
    "bounds = [0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8] \n",
    "\n",
    "def plt_results(model, model_name, X_test, y_test, fig_name):\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of predicted vs. actual values.\n",
    "\n",
    "    Parameters:\n",
    "        model: The trained model.\n",
    "        X_test (np.ndarray): Test features.\n",
    "        y_test (np.ndarray or pandas.Series): True labels of the test dataset.\n",
    "        fig_name (str): Filename to save the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mbe = np.mean(y_test - y_pred)\n",
    "    nse = 1 - (sum((y_test - y_pred) ** 2) / sum((y_test - np.mean(y_test)) ** 2))\n",
    "    kge = 1 - np.sqrt((r2_score(y_test, y_pred) - 1) ** 2 + (np.std(y_pred) / np.std(y_test) - 1) ** 2 + (np.mean(y_pred) / np.mean(y_test) - 1) ** 2)\n",
    "    si = rmse / np.mean(y_test)\n",
    "\n",
    "    plt.figure(figsize=(16, 16),dpi=800)\n",
    "    norm = Normalize(vmin=min(bounds), vmax=max(bounds))\n",
    "    scatter = plt.scatter(y_test, y_pred, c=y_test, cmap='jet', norm=norm)\n",
    "    plt.colorbar(scatter, ticks=bounds, boundaries=bounds)\n",
    "    \n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'purple', linestyle='--', label='Predicted Y = Observed Y')\n",
    "\n",
    "    reg = LinearRegression().fit(y_test.reshape(-1, 1), y_pred)\n",
    "    y_lin_pred = reg.predict(y_test.reshape(-1, 1))\n",
    "    plt.plot(y_test, y_lin_pred, color='red', linestyle='-', label='Fitted line')\n",
    "\n",
    "\n",
    "    se = np.sqrt(np.sum((y_pred - y_lin_pred) ** 2) / (len(y_test) - 2))\n",
    "    t_val = 1.96  # The 95% confidence interval will be added to visualize the model's uncertainty or prediction interval.\n",
    "    ci = t_val * se * np.sqrt(1 / len(y_test) + (y_test - np.mean(y_test))**2 / np.sum((y_test - np.mean(y_test))**2))\n",
    "    sorted_indices = np.argsort(y_test)\n",
    "    plt.fill_between(y_test[sorted_indices], \n",
    "                    y_lin_pred[sorted_indices] - ci[sorted_indices], \n",
    "                    y_lin_pred[sorted_indices] + ci[sorted_indices], \n",
    "                    color='pink', alpha=0.5, label='95% Confidence Interval')\n",
    "\n",
    "    # 显示线性拟合方程和R2\n",
    "    plt.text(min(y_test), (max(y_test) - min(y_test)) / 2, f'Y={reg.intercept_:.4f}+{reg.coef_[0]:.4f}X\\n$R^2$={r2_score(y_test, y_pred):.3f}', \n",
    "             fontsize=24, color='black', verticalalignment='center', horizontalalignment='left', bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    # 显示评估结果\n",
    "    plt.text(max(y_test), min(y_pred), f'WIA={1 - sum((y_test - y_pred) ** 2) / sum((np.abs(y_pred - np.mean(y_test)) + np.abs(y_test - np.mean(y_test))) ** 2):.3f}\\n'\n",
    "                                       f'MAE={mae:.3f}\\n'\n",
    "                                       f'MBE={mbe:.3f}\\n'\n",
    "                                       f'KGE={kge:.3f}\\n'\n",
    "                                       f'NSE={nse:.3f}\\n'\n",
    "                                       f'SI={si:.3f}\\n'\n",
    "                                       f'RMSE={rmse:.3f}',\n",
    "             fontsize=15, color='black', horizontalalignment='right', verticalalignment='bottom')\n",
    "\n",
    "\n",
    "    plt.xlabel('Observed values', fontsize=36)\n",
    "    plt.ylabel(f'{model_name} Predicted values', fontsize=36)\n",
    "    plt.xticks(fontsize=24)\n",
    "    plt.yticks(fontsize=24)\n",
    "    plt.legend(fontsize=20) \n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(fig_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "estimator_name = ['ElasticNet','Lasso', 'KNN', 'XGBoost', 'RandomForest','AdaBoost', 'ANN']\n",
    "\n",
    "\n",
    "for model_name in estimator_name:\n",
    "    filename = f'./df3_{model_name}.pkl'\n",
    "    estimator = joblib.load(filename)  \n",
    "    fig_name = f'./{model_name}_prediction.png'\n",
    "    plt_results(estimator, model_name, X_test, y_test, fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Use SHAP to Interpret the Random Forest (RF) and Perform Knowledge Discovery\n",
    "\n",
    "### **Detailed Steps**:\n",
    "\n",
    "1. **Generate SHAP Values**:\n",
    "   - Use the SHAP library to compute SHAP values for the Random Forest model.\n",
    "   - SHAP values quantify the contribution of each feature to the model's predictions.\n",
    "\n",
    "2. **Global Interpretation**:\n",
    "   - **Summary Plot**: Visualize the average impact of each feature across all samples to identify the most influential features.\n",
    "   - **Feature Importance**: Aggregate SHAP values to rank features based on their contribution to the model's predictions.\n",
    "\n",
    "3. **Local Interpretation**:\n",
    "   - **Force Plots**: Create visualizations for individual predictions to understand why the model made a specific decision.\n",
    "   - **Dependence Plot**: Explore interactions between a specific feature and the target variable while accounting for other features.\n",
    "\n",
    "4. **Knowledge Discovery**:\n",
    "   - Extract insights from feature contributions:\n",
    "     - Identify **key drivers** influencing the target variable.\n",
    "     - Uncover **non-linear relationships** and **interaction effects** between features.\n",
    "   - Use the findings to refine hypotheses or guide domain-specific interventions.\n",
    "\n",
    "5. **Integration with Domain Expertise**:\n",
    "   - Cross-validate SHAP insights with domain knowledge to ensure interpretability aligns with real-world phenomena.\n",
    "   - Use insights for feature engineering or decision-making processes.\n",
    "\n",
    "By leveraging SHAP, the Random Forest model becomes transparent, enabling both interpretability and actionable insights for improved decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "from shapash.data.data_loader import data_loading\n",
    "import shap\n",
    "shap.initjs()   \n",
    "from shapash.explainer.smart_explainer import SmartExplainer\n",
    "\n",
    "\n",
    "plt.rcParams ['font.sans-serif'] = ['Times new roman']    \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "RF_model = joblib.load(\"./df3_RandomForest.pkl\")\n",
    "\n",
    "X = df3[df3.columns[:-1]]\n",
    "y = df3[df3.columns[-1]]\n",
    "\n",
    "y_pre = pd.DataFrame({'y-pre': RF_model.predict(X)}, index=X.index)\n",
    "explainer = shap.TreeExplainer(RF_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Summary Plot (beeswarm)\n",
    "shap.summary_plot(shap_values, X, plot_type=\"dot\")\n",
    "plt.show()\n",
    "\n",
    "# Summary Plot (bar)\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Dependence Plot\n",
    "for feature in X.columns:\n",
    "    shap.dependence_plot(feature, shap_values, X)\n",
    "    plt.show()\n",
    "\n",
    "# Interaction Plot\n",
    "for feature in X.columns:\n",
    "    shap.dependence_plot(feature, shap_values, X, interaction_index='auto')\n",
    "    plt.show()\n",
    "\n",
    "# heatmap\n",
    "shap.summary_plot(shap_values, X, plot_type=\"heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Waterfall Plot for the first instance\n",
    "shap.waterfall_plot(shap.Explanation(values=shap_values[0], base_values=explainer.expected_value, data=X.iloc[0]))\n",
    "plt.show()\n",
    "\n",
    "# Force Plot for the first instance\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X.iloc[0])\n",
    "shap.save_html('./shap_force.html', shap.force_plot(explainer.expected_value, shap_values[0], X.iloc[0]))\n",
    "\n",
    "# Decision Plot\n",
    "shap.decision_plot(explainer.expected_value, shap_values, X)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
